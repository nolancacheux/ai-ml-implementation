Poids / Weight :
Les poids sont des paramètres appris lors de l'entraînement du réseau de neurones. Ils définissent l'importance de chaque entrée pour un neurone.

Biais :
Le biais est également un paramètre appris durant l'entraînement. Il permet de décaler la fonction d'activation et de gérer les cas où toutes les entrées seraient nulles.

Ces deux paramètres (poids et biais) sont constants après l'entraînement.

Fonction d'activation :
Elle transforme la somme pondérée des entrées en sortie. Exemples de fonctions d'activation :

Échelon de Heaviside :
Si l'entrée < 0 → sortie = 0 ;
Si l'entrée > 0 → sortie = 1.

Sign :
La sortie dépend uniquement du signe de l'entrée.

Linéaire :
Utilisée pour des tâches de régression, exemple pour afficher des valeurs comme la température.

Linéaire par morceaux :
Donne une importance aux petites valeurs, atténue les grandes valeurs.

Logistique :
Produite par la fonction sigmoïde.

Hyperbolique :
Fonction linéaire sur un petit intervalle proche de 0, convergeant vers -1 pour les petites valeurs et 1 pour les grandes valeurs. Utilisée souvent dans les couches internes des réseaux de neurones.

ReLU (Rectified Linear Unit) :
La sortie est linéaire si l'entrée > 0, et 0 si l'entrée < 0. C'est l'une des fonctions les plus puissantes en raison de sa simplicité et de ses performances en pratique.

Architecture d'un réseau de neurones :
Les réseaux de neurones sont composés de différentes couches :

Couche d'entrée :
Reçoit les caractéristiques d'entrée (par exemple, la largeur et la longueur des pétales d'une fleur pour classer si c'est une Setosa, Virginica, ou Versicolor).

Couches cachées :
Ce sont les couches intermédiaires où se font les calculs. Chaque neurone y effectue une tâche spécifique. Les couches cachées permettent de décomposer le problème en sous-problèmes.

Couche de sortie :
Produit le résultat final, par exemple la classe de la fleur.

Exemple avec TensorFlow :

Couche cachée dense avec 2 neurones et fonction d'activation ReLU.
Couche de sortie dense avec fonction d'activation sigmoïde.
Exemple d'un neurone en Python :

python
def neuron(x0, x1, x2, w0, w1, w2):
    return activation_function(w0 * x0 + w1 * x1 + w2 * x2)
Exemple de modélisation d'un réseau de neurones avec TensorFlow :

python
model = Sequential()
model.add(Dense(6, input_shape=(5,)))
Explication :
Si on a une couche dense avec 6 neurones en entrée, les paramètres sont calculés comme suit :

5 * 6 = 30 poids
6 biais
Total = 36 paramètres
En ajoutant une autre couche dense avec 7 neurones, cela donne :

6 * 7 = 42 poids
7 biais
Total = 49 paramètres
Le nombre total de paramètres peut être calculé pour chaque couche. Un réseau simple de régression avec 1 neurone aurait seulement 2 paramètres (A et B dans une équation de type ax + b).

Compilation du modèle :
Cela consiste à allouer la mémoire nécessaire pour les calculs matriciels et à préparer le réseau pour l'entraînement.

Historique des grands modèles de réseaux de neurones :

Modèle "Coca" de Google : 2.1 trillions de paramètres.
GPT-2 : 1.5 billions de paramètres.
Les modèles classiques d'ordinateurs se limitent souvent à GPT-2 en raison de la puissance de calcul nécessaire.
Lien : https://playground.tensorflow.org/

Concepts supplémentaires :

Epoch :
Une passe complète à travers le jeu de données d'entraînement.

Learning Rate :
Détermine à quelle vitesse le modèle ajuste les poids pendant l'entraînement.

Regularization :
Technique utilisée pour éviter le surapprentissage (overfitting).

Regularization Rate :
Paramètre pour ajuster la force de la régularisation.

Test Loss :
Mesure de l'erreur sur le jeu de données de test.

Algorithme de rétropropagation :
Utilisé pour ajuster les poids du réseau de neurones en fonction de l'erreur obtenue lors de la classification des données d'entraînement.

Chaque neurone a un rôle dans la classification et contribue au résultat final.
Pour visualiser ces concepts, vous pouvez utiliser le site de TensorFlow Playground.

Verbose :

Fait référence au niveau de détail des messages de sortie affichés pendant l'entraînement du modèle. En général, il s'agit d'un paramètre qui contrôle la quantité d'informations de débogage ou de progression affichées.

    Contexte :
        Dans les bibliothèques de machine learning comme TensorFlow, Keras, et Scikit-learn, le paramètre verbose est souvent utilisé pour spécifier si l'utilisateur souhaite voir des messages détaillés pendant l'entraînement du modèle.
    
    Niveaux de verbose :
        verbose=0 : Pas de sortie (mode silencieux).
        verbose=1 : Affiche une barre de progression et des informations de base sur chaque époque.
        verbose=2 : Affiche des informations détaillées sur chaque époque et chaque mini-lot.

Une époque (ou epoch en anglais) est une itération complète sur l'ensemble du jeu de données d'entraînement. En d'autres termes, une époque signifie que l'algorithme d'apprentissage a vu chaque exemple du jeu de données une fois.

https://adamharley.com/nn_vis/cnn/3d.html pour déterminer des nombres avec deep learning

https://bbycroft.net/llm : LLM Visualisation of NanoGpt , GPT2, GPT3...

Deux couches de 1000 neurones chacune représentent une allocation 
de 1 000 000 unités, avec 8 octets par unité, soit un total de 8 Mo
3 couches -> 16 Mo (2 paquets de 8 Mo)

Calcul du nombre de paramètres pour un réseau de neurones : 

        / 1000 --- 1000 \
x (=1) .                 .  y (=1)
        \ 1000 --- 1000 /
  
  2000  1001000  1001   = 1004001

2000 = 1 * 1000 + (Biais = 1000)
1001000 = 1000*1000 + (Biais = 1000)
1001 = 1000 * 1 +  (Biais = 1)


NN for regression : Linear
NN for Classification : 
    Binary classification : One node : Sigmoid ( like probability , between 0 and 1 function)
    Multiclass Classification : Softmax Activation (when you search for multiple things , example a dog , a cat , a horse in a photo)
    Multilabel Classification : Sigmoid Activation
