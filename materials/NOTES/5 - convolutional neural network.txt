CNN : https://adamharley.com/nn_vis/cnn/3d.html pour d√©terminer des nombres avec deep learning CNN

DEMONSTRATION DE LA CONVOLUTION : https://deeplizard.com/resource/pavq7noze2

CNN EXPLAINER : https://poloclub.github.io/cnn-explainer/

EXEMPLE DE CODE CNN POUR MNIST : 

from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense
from tensorflow.keras.models import Sequential

model = Sequential()  # Initialize the Sequential model

# Reshape the input data to 28x28x1
x_train = x_train.reshape(-1, 28, 28, 1)  # Reshape training data to 28x28 pixels with 1 channel (grayscale)
x_test = x_test.reshape(-1, 28, 28, 1)    # Reshape test data to 28x28 pixels with 1 channel (grayscale)

# Add convolutional layers

# filters = kernels
model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))  # Add a Conv2D layer with 6 filters, 3x3 kernel size, ReLU activation, and input shape of 28x28x1
model.add(AveragePooling2D(pool_size=(2, 2)))  # Add an AveragePooling2D layer with 2x2 pool size
model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))  # Add another Conv2D layer with 16 filters, 3x3 kernel size, and ReLU activation
model.add(AveragePooling2D(pool_size=(2, 2)))  # Add another AveragePooling2D layer with 2x2 pool size
model.add(Flatten())  # Flatten the output from the convolutional layers to feed into the dense layers

model.add(Dense(units=128, activation='relu'))  # Add a Dense layer with 128 units and ReLU activation
model.add(Dense(units=84, activation='relu'))  # Add another Dense layer with 84 units and ReLU activation
model.add(Dense(units=num_classes, activation='softmax'))  # Add the output Dense layer with units equal to the number of classes and softmax activation

model.summary()  # Print the summary of the model